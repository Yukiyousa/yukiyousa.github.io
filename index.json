[{"categories":["Notes"],"content":"1.Meta-Learning ","date":"2021-10-23","objectID":"/maml-introduction/:0:0","tags":["深度学习","笔记"],"title":"Model-Agnostic Meta-Learning（MAML）介绍与算法分析","uri":"/maml-introduction/"},{"categories":["Notes"],"content":"1.1介绍 常见的深度学习模型，目的是学习一个用于预测的网络模型，比如能够实现猫狗分类的模型，我们学习训练的是模型的参数，学习的目的是让最终训练好的参数在数据集上的准确率越高，但 Meta Learning 面向是的学习的过程而非结果，如我们在学习英语时，若是模仿apple 的发音，我们能很快的学会说这个单词，但当我们碰到新的单词如 banana 时会无从下手，这就是我们学习音标的原因，学习音标的过程正是元学习的体现 （1）Meta Learning 中文翻译为“元学习”，它研究的不是如何提升模型解决某项具体的任务（分类，回归，检测）的能力，而是研究如何提升模型解决一系列任务的能力；Meta learning 即 learn to learn ，我们希望机器学习怎样去学习这件事情，就是学会语音辨识、图像辨识以后，它学会了如何去学习学习这件事情，而不是停留在语音和图像的任务上 （2）我们希望通过 meta-learning 学习出一个非常好的模型初始化参数，有了这个初始化参数后，我们只需要少量的样本就可以快速在这个模型中进行收敛 meta-learning 在 few-shot 中经常使用，因为 meta learning 的训练任务很多，因此每个任务的样本很少 （3）在 meta-learning 中，把学习算法也当作一个函数 F ，吃进去的是训练资料，吐出来另外一个function，要找一个函数 F，输入是训练资料输出是模型 目的 输入 函数 输出 机器学习 通过训练数据学习输入X与输出Y之间的映射，找到函数f X f Y 元学习 通过task找到函数F，F可以输出一个函数f，f用于新的任务 task F f ","date":"2021-10-23","objectID":"/maml-introduction/:1:0","tags":["深度学习","笔记"],"title":"Model-Agnostic Meta-Learning（MAML）介绍与算法分析","uri":"/maml-introduction/"},{"categories":["Notes"],"content":"1.2训练 首先定义 F 的损失函数（N 表示 N 个 task ，ln 表示第 n 个 task 的损失） $$ L(F)=\\sum_{n=1}^{N}l^{n} $$ 找一个最好的 F* 使 Loss 最小，得到学习算法 F* 测试时我们首先用 testing set 的 support set 去让 F* 找出 f* ，然后使用 f* 来对 testing set 的 query set 进行测试 \r","date":"2021-10-23","objectID":"/maml-introduction/:2:0","tags":["深度学习","笔记"],"title":"Model-Agnostic Meta-Learning（MAML）介绍与算法分析","uri":"/maml-introduction/"},{"categories":["Notes"],"content":"1.3相关概念 ","date":"2021-10-23","objectID":"/maml-introduction/:3:0","tags":["深度学习","笔记"],"title":"Model-Agnostic Meta-Learning（MAML）介绍与算法分析","uri":"/maml-introduction/"},{"categories":["Notes"],"content":"1.3.1 N-way K-shot 在 few-shot learning 中有一个术语叫做 N-way K-shot 问题，简单的说就是我们需要分类的样本属于 N 个类中一种，但是我们每个类训练集中的样本只有 K 个，即一共只有 N ∗ K 个样本的类别是已知的 ","date":"2021-10-23","objectID":"/maml-introduction/:3:1","tags":["深度学习","笔记"],"title":"Model-Agnostic Meta-Learning（MAML）介绍与算法分析","uri":"/maml-introduction/"},{"categories":["Notes"],"content":"1.3.2 task 既然是学会学习，那么输入就不是简简单单的数据了，而是一组一组的 task ，比如猫狗分类、水果分类等等都是一个个 task ，那么这些 task 应该如何分类呢？ 在 machine learning 中，准备训练集和测试集，在 meta learning 中，准备训练任务及其对应的资料 在训练中将 task 划分为 training tasks 和 testing tasks 在few-shot中，通常把上述的 tasks 中的 train 和 test 称为 support set 和 query set 。 \r","date":"2021-10-23","objectID":"/maml-introduction/:3:2","tags":["深度学习","笔记"],"title":"Model-Agnostic Meta-Learning（MAML）介绍与算法分析","uri":"/maml-introduction/"},{"categories":["Notes"],"content":"1.3.3 Model-Agnostic Model-Agnostic 即模型无关，这里的模型无关意思是 MAML 可以用于 CNN RNN 甚至 RL 中，绝大多数深度学习模型都可以作为 base-learner 无缝嵌入MAML中 2.MAML ","date":"2021-10-23","objectID":"/maml-introduction/:3:3","tags":["深度学习","笔记"],"title":"Model-Agnostic Meta-Learning（MAML）介绍与算法分析","uri":"/maml-introduction/"},{"categories":["Notes"],"content":"2.1基本思想 （1）目的：学 MAML 的思想是学习一套初始化参数 ，这个初始化参数在遇到新的问题时，只需要使用少量的样本 (few-shot learning) 进行几步梯度下降就可以取得很好地效果 （2）方法：定义损失函数，提供一套初始化参数，然后让网络在不同 task 上去训练，最终得到属于不同task的网络参数，然后评估这些不同的网络参数在各自 task 上的测试集里表现如何，得知最初的这套初始化参数到底怎么样 \r","date":"2021-10-23","objectID":"/maml-introduction/:4:0","tags":["深度学习","笔记"],"title":"Model-Agnostic Meta-Learning（MAML）介绍与算法分析","uri":"/maml-introduction/"},{"categories":["Notes"],"content":"2.2MAML与model Pre-training model pre-training 和 MAML 的区别是损失是不同的。 MAML找一组初始化参数经过训练得到好的结果（潜力） model pre-training 找一组初始化参数在所有的task上得到好的效果（现在表现的效果） \r","date":"2021-10-23","objectID":"/maml-introduction/:5:0","tags":["深度学习","笔记"],"title":"Model-Agnostic Meta-Learning（MAML）介绍与算法分析","uri":"/maml-introduction/"},{"categories":["Notes"],"content":"2.3算法流程 MAML中是存在两种梯度下降的，也就是gradient by gradient。第一种梯度下降是每个task都会执行的，而第二种梯度下降只有等batch size个task全部完成第一种梯度下降后才会执行的 \r以 5-way 5-shot 例子为例，算法流程如下 MAML在 task 上进行训练，training tasks 分为若干个 task ，每个 task 又区分了 support set 和 query set 我们用于训练的模型架构是 (初始化参数为)，假设是一个输出节点为 5 的CNN，训练的目的是为了使得模型有较优秀的初始化参数 我们将1个任务task的support set去训练 ，这里进行第一种梯度下降，也就是 。在执行第2个task训练时，有 。执行第 batch size 个 task 后，有 上述步骤3用了 batch size 个 task 对 进行了训练，然后我们使用上述batch个task中地query set去测试参数为，获得总损失函数 ，这个损失函数就是一个batch task中每个task的query set的损失函数之和 获得总损失函数后，我们就要对其进行第二种的梯度下降。即更新初始化参数 ，也就是 来更新初始化参数。这样不断地从步骤3开始训练，最终能够在数据集上获得该模型比较好的初始化参数（若无法对进行学习，使用强化学习训练) 关于以上过程补充以下解释：假设原模型为，我们复制它得到，在上通过反向传播更新参数，得到第一次梯度更新结果，在结果上进行第二次梯度更新，此时需要在结果上计算梯度，但梯度更新的并不是，而是原模型 参考资料 [1] Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks [2] Model-Agnostic Meta-Learning （MAML）模型介绍及算法详解 [3] [meta-learning] 对MAML的深度解析 [4] 一文入门元学习（Meta-Learning）（附代码） ","date":"2021-10-23","objectID":"/maml-introduction/:6:0","tags":["深度学习","笔记"],"title":"Model-Agnostic Meta-Learning（MAML）介绍与算法分析","uri":"/maml-introduction/"},{"categories":["Notes"],"content":"之前用 typecho 搭的博客需要使用服务器，续不起阿里云服务器了（留下了贫穷的泪水，于是发现了好用的 Hugo ~ 1.Hugo初探 Hugo 是由 Go 语言实现的静态网站生成器。简单、易用、高效、易扩展、快速部署！ ","date":"2021-10-17","objectID":"/hugo-githubpages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/:0:0","tags":["笔记"],"title":"Hugo+GithubPages搭建博客教程","uri":"/hugo-githubpages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"},{"categories":["Notes"],"content":"1.1安装Hugo 首先我们需要下载 Hugo 和 git，前往Hugo中文文档，推荐使用二进制安装，点击Hugo Releases进入github页面，选择对应版本下载 下载完成后解压到本地合适的文件夹，假设我们解压到 D:\\myblog\\bin，将此路径添加到系统环境变量中，打开终端输入 hugo version 检查hugo版本，查询成功则表示安装成功 hugo version ","date":"2021-10-17","objectID":"/hugo-githubpages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/:1:0","tags":["笔记"],"title":"Hugo+GithubPages搭建博客教程","uri":"/hugo-githubpages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"},{"categories":["Notes"],"content":"1.2生成站点 选择合适的文件夹建站，使用 Hugo 快速生成站点，进入博客根目录输入（.代表当前目录） hugo new site demosite # 命令格式，hugo new site \u003c项目名称\u003e # or hugo new site . 进入根目录，站点目录结构如下： ├── archetypes │ └── default.md ├── content ├── layouts ├── data ├── resources ├── static ├── themes # 主题文件夹 └── config.coml # 配置文件 ","date":"2021-10-17","objectID":"/hugo-githubpages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/:2:0","tags":["笔记"],"title":"Hugo+GithubPages搭建博客教程","uri":"/hugo-githubpages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"},{"categories":["Notes"],"content":"1.3主题选择 前往Hugo主题选择心仪的主题，找到对应 Github 仓库，下载zip或直接使用 git clone cd themes git clone https://github.com/xxx.git 若主题文件夹含有 -master 后缀，尽量将其删除，以免后期引起奇奇怪怪的BUG，主题的不同也可能需要在初期配置不同的文件，本文主要面向Lovelt主题 重要！！！将主题的 config.coml 文件替换根目录下的同名文件，此文件是整个站点主题的配置文件，可根据自己的需求调整 ","date":"2021-10-17","objectID":"/hugo-githubpages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/:3:0","tags":["笔记"],"title":"Hugo+GithubPages搭建博客教程","uri":"/hugo-githubpages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"},{"categories":["Notes"],"content":"1.4创建文章 让我们创建第一篇文章！ hugo new posts/demo.md ","date":"2021-10-17","objectID":"/hugo-githubpages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/:4:0","tags":["笔记"],"title":"Hugo+GithubPages搭建博客教程","uri":"/hugo-githubpages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"},{"categories":["Notes"],"content":"1.5启动服务 启动 hugo 服务器，进入 http://localhost:1313 预览页面 hugo server 2.Lovelt主题配置记录 本部分Lovelt主题踩坑配置记录 ","date":"2021-10-17","objectID":"/hugo-githubpages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/:5:0","tags":["笔记"],"title":"Hugo+GithubPages搭建博客教程","uri":"/hugo-githubpages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"},{"categories":["Notes"],"content":"2.1初始配置 在最开始配置前需要修改一下主题路径 themesDir 配置，将其注释掉 baseURL = \"https://yourname.github.io\" # [en, zh-cn, fr, pl, ...] determines default content language # [en, zh-cn, fr, pl, ...] 设置默认的语言 defaultContentLanguage = \"zh-cn\" # theme # 主题 theme = \"LoveIt\" # themes directory # 主题目录 themesDir = \"../..\" # 将其注释 # website title # 网站标题 title = \"\" ","date":"2021-10-17","objectID":"/hugo-githubpages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/:6:0","tags":["笔记"],"title":"Hugo+GithubPages搭建博客教程","uri":"/hugo-githubpages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"},{"categories":["Notes"],"content":"2.2时间设置 最开始我创建博客在设置时间时，时间变成了 0001-01-01 。Hugo 在生成静态页面的时候，不会生成超过当前时间的文章；而 Hugo 默认采用的是 格林尼治平时 (GMT)，比北京时间 (UTC+8) 晚了 8 个小时。也就是说，当北京时间在 08:00 之前，而你又将文章发布日期设在当天时，Hugo 就默认不会生成这个页面，可以在文章开头使用 timezone 指定时区： --- ... date: 2019-05-12 timezone: UTC+8 ... --- 其他配置可参考 config.toml 进行个性化配置 3.GithubPages和个人域名 ","date":"2021-10-17","objectID":"/hugo-githubpages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/:7:0","tags":["笔记"],"title":"Hugo+GithubPages搭建博客教程","uri":"/hugo-githubpages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"},{"categories":["Notes"],"content":"3.1GithubPages建站 GitHubPages 是一项静态站点托管服务，它直接从 GitHub 上的仓库获取 index.html、HTML、CSS 和 JavaScript 文件，也可以通过构建过程运行文件，然后发布网站。我们使用 GitHubPages 搭建个人网站 （1）创建一个新的 Github 项目，项目名称需要是 \u003cusername.github.io\u003e格式，这里的 username 是个人 Github 的用户名，最好先不要选择创建 README.md 文件 （2）生成静态页面之前需要修改 config.toml 文件中的 baseURL 配置，将其修改为个人站点，即https://yourname.github.io # config.toml baseURL = \"https://yourname.github.io\" title = \"\" theme = \"\" （3）若要将博客托管在 Github 上，需要上传静态页面，在根目录下输入 hugo 可以生成静态页面 hugo 输入后我们发下根目录下多了一个 public 目录，所有静态页面都会生成到 public 目录 （4）上传项目，静态页面生成完成后，便可以将整个静态页面以及本项目其他文件推送到 Github 项目中 cd public git init git remote add origin https://github.com/yourname/xxx.github.io.git git add . git commit -m \"first commit\" git push -u origin master （5）浏览器访问 http://yourname.github.io/，你的博客已经超成功部署 ","date":"2021-10-17","objectID":"/hugo-githubpages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/:8:0","tags":["笔记"],"title":"Hugo+GithubPages搭建博客教程","uri":"/hugo-githubpages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"},{"categories":["Notes"],"content":"3.2配置个人域名 在配置个人域名之前，我们需要一些前置知识（以阿里云为例） 域名，简单来说就是访问博客的链接，我们需要购买自己的域名。可以通过阿里云选择域名 在阿里云购买域名过程中需要实名认证，实名认证后需要进行备案，可以通过阿里云APP方便操作。 DNS解析，简单来说就是解析域名和IP的对应关系。具体可参考阿里云域名购买与DNS解析教程。在cmd中输入 ping +域名 ，丢包率为0则解析成功~ （1）当你拥有域名后，首先是用 ping 命令找到存放你的 GitHubPages 的主机的IP地址，在终端里面用命令 ping yourname.github.io 便可完成，回复地址即我们要找的IP地址 （2）在购买域名的提供商为域名添加解析，添加两条记录 记录类型：CNAME 将一个域名指向另一个域名，需要添加 CNAME 记录（yourname.github.io）。 主机记录：www 表示访问域名的时候以 www 开头为一级域名。如果是二级域名的话就在前面加上自己想要的参数，访问的时候也是以二级域名的形式访问 记录类型：A 将域名指向一个IPv4地址，如果需要将域名指向一个 IP 地址（刚才我们 ping 获得的IP地址），就需要添加 A 记录。 主机记录：@ 表示访问的时候直接访问，前面不加任何参数 （3）在 Github 仓库根目录中创建 CNAME 文件，内容是你的域名（不要加http头），同步远程库同步到本地库。 git pull --rebase origin master （4）打开仓库 Repository Settings 的 Pages ，我们发现 Custom domain 变为我们的域名，若没有自行更改即可，选择 Enforce HTTPS 为你的网站添加小绿锁，配置后需要等一会才可以生效~ （5） 设置完成后就可以通过你的域名访问部署在GitHub上的Hugo的网站啦 ","date":"2021-10-17","objectID":"/hugo-githubpages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/:9:0","tags":["笔记"],"title":"Hugo+GithubPages搭建博客教程","uri":"/hugo-githubpages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B/"},{"categories":["Notes"],"content":"背景：第十四届全国大学生信息安全竞赛作品赛全国二等奖（团队队长） 信安大赛之于我而言，是一段从寒假到暑假的历程，期间看论文、想方向、实现作品、写文档、做视频，最后取得了还不错的成绩，在这里写点比赛总结~ 一、万事开头难 选题的方向真的很重要，不能选的太冷门，最好能结合近两年的技术热点，纵观不同领域选择合适的大方向，其次看历年的获奖作品也挺重要，通过对历年的获奖作品进行调研，总结出比赛倾向的作品主题，同时历年获奖作品也能够帮助拓展视野，不仅仅局限于自己的想法。选好了要做的方向或题目，就开始推进项目。 什么？你问我应该选什么题目？ \r二、背景调研与计划 既然参加是作品赛，那么作品要有明确的落地点，或针对某种问题或现象。在初期要确定好作品最终会服务于哪些领域或解决哪些问题。此外制定好计划也至关重要。 还有作品赛偏技术点，商业模式可以谈但并不是重点。 三、开始实现作品 在确定好题目的基础上，可以根据自己选的类型寻找相关领域的论文、书籍、博客，并对内容进行总结，在阅读论文的同时，会接触到大量的技术点、模型、方法等等，可以寻找适合的方法加以改进，应用到自己的作品中（如果是大佬全自创请忽略这句话Orz）。 在此基础上，就要干活啦。开始实现作品demo，组员们之间开会不断完善，撰写作品报告（重要！！！），可以边实现作品边写报告，不断进行增删查改。 四、第一关——初赛 跋山涉水终于要到初赛的截止日期了，这个阶段应该已经大致实现了作品的基础功能，并有了一份作品报告，接下来重要的事就是修饰报告，因为初赛只要求提交作品报告，当然你要做PPT拍视频也更好，但作品报告是重中之重，我认为需要注意以下几点： （1）格式一定要按照给定的模板写报告。 （2）插图、表格等。尽量做好看点，插图表格一定要标好号，图片标号在下方，表格在上方。 （3）目录标题，好的目录标题可以让你的报告更加清晰。 （4）注意文章引用格式和参考文献 （5）其他注意事项。 五、通过初赛进入决赛 本来是要去青岛旅游参加决赛的最后没去成 在等待初赛结果的时间内可以休息一下，成功通过了初赛网评，入围了决赛，开始进入决赛阶段。决赛主要分为作品PPT展示与作品展示，在决赛前可以对自己的作品进行优化，同时继续完善作品报告并制作作品PPT，作品展示推荐通过视频的方式进行展示，尽最大可能降低意外风险。 答辩前多演练演练，写一个答辩稿子准备下可能问的问题，答辩的时候放松一点，一定要诚实，不然可能会被问住。 六、拿奖（ 回想准备比赛的历程也蛮累的，过程中换题目、改报告格式、拍视频给视频配音很多事。进入初赛后，暑假一段时间没回家和队友一起完善作品，一个人在宿舍也挺孤独的（泪 共勉 \r更新于2021/10/29 ！！！感谢山东大学🎉 ","date":"2021-10-13","objectID":"/ciscn2021-summary/:0:0","tags":["笔记"],"title":"全国大学生信息安全竞赛作品赛（CISCN2021）总结","uri":"/ciscn2021-summary/"},{"categories":["Notes"],"content":"1.损失函数 损失函数是衡量模型输出与真实情况差异的标准，我们所说的优化即优化网络权值使得损失函数变得更小，一般而言在机器学习中往往有三个概念损失函数、代价函数、目标函数 Loss Function：计算一个样本的一个差异 Loss = f(y^, y) Cost Function：计算整个训练集Loss的一个平均值 Objective Function：在机器学习模型训练中，这是最终的目标，过拟合和欠拟合进行权衡 ","date":"2021-10-09","objectID":"/pytorch-note5/:0:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（五）损失函数与优化器","uri":"/pytorch-note5/"},{"categories":["Notes"],"content":"1.1交叉熵损失CrossEntropyLoss nn.CrossEntropyLoss是nn.LogSortmax()与nn.NLLLoss()结合，进行交叉熵计算，交叉熵=信息熵+相对熵 ","date":"2021-10-09","objectID":"/pytorch-note5/:1:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（五）损失函数与优化器","uri":"/pytorch-note5/"},{"categories":["Notes"],"content":"1.2其他常用损失函数 TripletMarginLoss：三元组损失函数，目的是让Postive元和Anchor元之间的距离尽可能的小，Postive元和Negative元之间的距离尽可能的大。 2.优化器 在机器学习中可分为五大步骤：数据 -\u003e 模型 -\u003e 损失 -\u003e 优化器 -\u003e 迭代训练。我们在前向传播的过程中，得到了模型输出与真实标签的差异称为损失，有了损失我们通过反向传播更新梯度，接下来的过程中，我们需要优化器进行梯度下降，优化器根据梯度更新参数，使得损失不断降低。 ","date":"2021-10-09","objectID":"/pytorch-note5/:2:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（五）损失函数与优化器","uri":"/pytorch-note5/"},{"categories":["Notes"],"content":"2.1什么是优化器 优化器是管理并更新模型中可学习参数的值， 使得模型输出更接近真实标签。 ","date":"2021-10-09","objectID":"/pytorch-note5/:3:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（五）损失函数与优化器","uri":"/pytorch-note5/"},{"categories":["Notes"],"content":"2.2优化器基类Optimizer ","date":"2021-10-09","objectID":"/pytorch-note5/:4:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（五）损失函数与优化器","uri":"/pytorch-note5/"},{"categories":["Notes"],"content":"2.2.1基本属性 Optimizer优化器的基本属性： defaults: 优化器超参数，里面会存储一些学习了， momentum的值，衰减系数等 state: 参数的缓存， 如momentum的缓存（使用前几次梯度进行平均） param_groups: 管理的参数组， 是个列表，每一个元素是一个字典，在字典中有key，key里面的值才是我们真正的参数（这个很重要， 进行参数管理） _step_count: 记录更新次数， 学习率调整中使用， 如迭代100次之后更新学习率的时候，就得记录这里的100. ","date":"2021-10-09","objectID":"/pytorch-note5/:4:1","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（五）损失函数与优化器","uri":"/pytorch-note5/"},{"categories":["Notes"],"content":"2.2.2基本方法 优化器的基本方法 zero_grad()：清空所管理参数的梯度，PyTorch一个特性是张量梯度不自动清零 step()：执行一步更新，其中可传入参数 closure（一个闭包）。 add_param_group()：添加参数组，对优化器的参数进行分组管理，对不同组的参数可以设置不同的超参数 state_dict()：获取优化器当前状态信息字典，key 是各层参数名，value 就是参数 load_state_dict()：加载状态信息字典，将 state_dict 中的参数加载到当前网络，常用于 finetune for input, target in dataset: def closure(): optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) loss.backward() return loss optimizer.step(closure) ","date":"2021-10-09","objectID":"/pytorch-note5/:4:2","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（五）损失函数与优化器","uri":"/pytorch-note5/"},{"categories":["Notes"],"content":"2.3常用优化器 optim.SGD: 随机梯度下降法 optim.Adagrad: 自适应学习率梯度下降法 optim.RMSprop: Adagrad的改进 optim.Adadelta: Adagrad的改进 optim.Adam: RMSprop结合Momentum optim.Adamax: Adam增加学习率上限 optim.SparseAdam: 稀疏版的Adam optim.ASGD: 随机平均梯度下降 optim.Rprop: 弹性反向传播 optim.LBFGS: BFGS的改进 ","date":"2021-10-09","objectID":"/pytorch-note5/:5:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（五）损失函数与优化器","uri":"/pytorch-note5/"},{"categories":["Notes"],"content":"2.4学习率调整策略 在优化器当中有很多超参数，例如学习率，动量系数等，这里面最重要的一个参数就是学习率。它直接控制了参数更新步伐的大小，整个训练当中，学习率也不是一成不变的，也可以调整和变化。 ","date":"2021-10-09","objectID":"/pytorch-note5/:6:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（五）损失函数与优化器","uri":"/pytorch-note5/"},{"categories":["Notes"],"content":"2.4.1为什么要调整学习率 在训练模型时，一般一开始学习率比较大，这样可以以较快速度达到最优点附近，再将学习率调低，缓慢接近收敛值 ","date":"2021-10-09","objectID":"/pytorch-note5/:6:1","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（五）损失函数与优化器","uri":"/pytorch-note5/"},{"categories":["Notes"],"content":"2.4.2学习率调整策略 StepLR ：等间隔调整学习率 MultiStepLR：按给定间隔调整学习率 ExponentialLR：按指数衰减调整学习率 CosineAnnealingLR：余弦周期调整学习率 ReduceLRonPlateau：监控指标， 当指标不再变化则调整， 这个非常实用。可以监控loss或者准确率，当不在变化的时候，我们再去调整。 LambdaLR：自定义调整策略 ","date":"2021-10-09","objectID":"/pytorch-note5/:6:2","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（五）损失函数与优化器","uri":"/pytorch-note5/"},{"categories":["Notes"],"content":"2.4.3分类 PyTorch提供了六种学习率调整策略，可分为三大类 有序调整：等间隔下降(Step)， 按需设定下降间隔(MultiStep)，指数下降(Exponential)和 CosineAnnealing 自适应调整：依训练状况伺机调整，这就是 ReduceLROnPlateau 方法 自定义调整：Lambda 方法提供的调整策略十分灵活，我们可以为不同的层设定不同的学习率调整方法，这在 fine-tune （微调）中十分有用 ","date":"2021-10-09","objectID":"/pytorch-note5/:6:3","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（五）损失函数与优化器","uri":"/pytorch-note5/"},{"categories":["Notes"],"content":"1.模型构建步骤 ","date":"2021-10-05","objectID":"/pytorch-note4/:0:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（四）模型搭建与权值初始化","uri":"/pytorch-note4/"},{"categories":["Notes"],"content":"1.1两大模块 构建子模块：卷积层、池化层、全连接层，自己建立的模型（继承nn.Module）的__init__()方法 拼接子模块：构建了子模块，将子模块按一定的顺序排序，逻辑拼接得到最终的网络模型，在模型的forward()方法中 ","date":"2021-10-05","objectID":"/pytorch-note4/:1:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（四）模型搭建与权值初始化","uri":"/pytorch-note4/"},{"categories":["Notes"],"content":"1.2具体步骤 首先，必须继承 nn.Module 这个类，要让 PyTorch 知道这个类是一个 Module。 其次，在 init(self) 中设置好需要的“组件\"(如 conv、pooling、Linear、BatchNorm 等)。 最后，在 forward(self, x) 中用定义好的“组件”进行组装，就像搭积木把网络结构搭建 出来，这样一个模型就定义好了。 2.nn.Module类 torch.nn是神经网络模块，有以下子模块 nn.Parameter：张量子类，表示可学习参数、如weight，bias nn.Module：所有网络基类，管理网络属性 nn.functional：函数具体实现，如卷积、池化、激活函数等 nn.init：参数初始化方法 nn.Module是所有网络层的基类，管理有关网络的属性。 ","date":"2021-10-05","objectID":"/pytorch-note4/:2:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（四）模型搭建与权值初始化","uri":"/pytorch-note4/"},{"categories":["Notes"],"content":"2.1nn.Parameter PyTorch一般将参数用nn.Parameter来表示，并且用nn.Module来管理其结构下的所有参数。 ","date":"2021-10-05","objectID":"/pytorch-note4/:3:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（四）模型搭建与权值初始化","uri":"/pytorch-note4/"},{"categories":["Notes"],"content":"2.2nn.functional nn.functional(一般引入后改名为F)有各种功能组件的函数实现，为便于对参数管理，一般通过nn.Module转换为类的实现形式，封装在nn模块下： 激活函数变成(nn.ReLu, nn.Sigmoid, nn.Tanh, nn.Softmax) 模型层(nn.Linear, nn.Conv2d, nn.MaxPool2d, nn.Embedding) 损失函数(nn.BCELoss, nn.MSELoss, nn.CrossEntorpyLoss) ","date":"2021-10-05","objectID":"/pytorch-note4/:4:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（四）模型搭建与权值初始化","uri":"/pytorch-note4/"},{"categories":["Notes"],"content":"2.3nn.Module nn.Module是所有网络层的基类，管理有关网络的属性，在其中有8个重要的属性 3.nn.Sequential torch.nn.Sequential 其实就是 Sequential 容器，该容器将一系列操作按先后顺序给包起来，方便重复使用 class LeNet(nn.Module): def __init__(self, classes): super(LeNetSequential, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 6, 5), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(6, 16, 5), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2),) self.classifier = nn.Sequential( nn.Linear(16*5*5, 120), nn.ReLU(), nn.Linear(120, 84), nn.ReLU(), nn.Linear(84, classes),) def forward(self, x): x = self.features(x) x = x.view(x.size()[0], -1) x = self.classifier(x) return x 模型定义就是先继承，再构建组件，最后组装。基本组件可从 torch.nn 中获取，同时为了方便重复使用组件，可以使用 Sequential 容器将一系列组件包起来，最后在 forward() 函数中将这些组件组装成模型。 4.nn网络层 ","date":"2021-10-05","objectID":"/pytorch-note4/:5:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（四）模型搭建与权值初始化","uri":"/pytorch-note4/"},{"categories":["Notes"],"content":"4.1卷积层 二位卷积层网络参数，n表示输入大小，f表示卷积核大小，p表示padding，s表示步长 nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding) 无padding: n - f / s + 1 有padding: n + 2p -f / s + 1 ","date":"2021-10-05","objectID":"/pytorch-note4/:6:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（四）模型搭建与权值初始化","uri":"/pytorch-note4/"},{"categories":["Notes"],"content":"4.2池化层 池化层是可以帮助我们剔除一些冗余像素 nn.MaxPool2d(kernel_size, stride, padding) nn.AvgPool2d(kernel_size, stride, padding) nn.MaxUnpool2d(kernel_size, stride, padding) ","date":"2021-10-05","objectID":"/pytorch-note4/:7:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（四）模型搭建与权值初始化","uri":"/pytorch-note4/"},{"categories":["Notes"],"content":"4.3线性层 线性层又称为全连接层，其每个神经元与上一层所有神经元相连实现对前一层的线性组合，线性变换 nn.Linear(in_features, out_features, bias=True) 5.权值初始化 在网络模型搭建完成之后，对网络中的权重进行合适的初始化是非常重要的一个步骤， 初始化好的情况下初始化在模型的最优解附近，模型训练速度提升，反之模型需要多次训练 ","date":"2021-10-05","objectID":"/pytorch-note4/:8:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（四）模型搭建与权值初始化","uri":"/pytorch-note4/"},{"categories":["Notes"],"content":"5.1权值初始化流程 第一步，先设定什么层用什么初始化方法，初始化方法在 torch.nn.init 中给出 第二步，实例化一个模型之后，执行该函数，即可完成初始化 ","date":"2021-10-05","objectID":"/pytorch-note4/:9:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（四）模型搭建与权值初始化","uri":"/pytorch-note4/"},{"categories":["Notes"],"content":"5.2常用初始化方法 ","date":"2021-10-05","objectID":"/pytorch-note4/:10:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（四）模型搭建与权值初始化","uri":"/pytorch-note4/"},{"categories":["Notes"],"content":"5.2.1Xavier初始化 （1）均匀分布：初始化方法服从均匀分布U(-a, a) torch.nn.init.xavier_uniform_(tensor, gain=1) （2）正态分布：初始化方法服从正太分布 torch.nn.init.xavier_normal_(tensor, gain=1) （3）使用方法 def initialize(self): for m in self.modules(): if isinstance(m, nn.Linear): # Xavier初始化权重 tanh_gain = nn.init.calculate_gain('tanh') nn.init.xavier_uniform_(m.weight.data, gain=tanh_gain) ","date":"2021-10-05","objectID":"/pytorch-note4/:10:1","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（四）模型搭建与权值初始化","uri":"/pytorch-note4/"},{"categories":["Notes"],"content":"5.2.2kaiming初始化 torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu') torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu') ","date":"2021-10-05","objectID":"/pytorch-note4/:10:2","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（四）模型搭建与权值初始化","uri":"/pytorch-note4/"},{"categories":["Notes"],"content":"1.数据读取机制 读取数据的基本流程是： 将存储图片的路径和标签信息转为list，该list可以通过index选取一个元素对应一个样本 通过getitem函数，读取数据和标签，并返回数据和标签 ","date":"2021-10-03","objectID":"/pytorch-note3/:0:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（三）数据读取机制与图像预处理模块","uri":"/pytorch-note3/"},{"categories":["Notes"],"content":"1.1Dataset ","date":"2021-10-03","objectID":"/pytorch-note3/:1:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（三）数据读取机制与图像预处理模块","uri":"/pytorch-note3/"},{"categories":["Notes"],"content":"1.1.1Dataset类 PyTorch通过Dataset类读取数据，Dataset类作为所有的datasets的基类存在，所有的dataset都要继承它，并且必须复写__getitem__()这个类方法。 def __getitem__(self, index): raise NotImplementedError def __len__(self): raise NotImplementedError __getitem__()接收一个 index，然后返回图片数据和标签，这个 index 通常指的是一个 list 的 index，这个 list 的每个元素就包含了图片数据的路径和标签信息。 ","date":"2021-10-03","objectID":"/pytorch-note3/:1:1","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（三）数据读取机制与图像预处理模块","uri":"/pytorch-note3/"},{"categories":["Notes"],"content":"1.1.2构建Dataset子类 我们构建Dataset子类——MyDataset类： class MyDataset(Dataset): def __init__(self, x, transform=None): self.data_info = x # data_info存储所有图片路径和标签，在DataLoader中通过index读取样本 self.transform = transform def __getitem__(self, index): path_img, label = self.x[index] img = Image.open(path_img).convert('RGB') # 0~255 if self.transform is not None: img = self.transform(img) # 在这里做transform，转为tensor等等 return img, label def __len__(self): return len(self.imgs) 在MyDataset中，主要获取图片的索引以及定义如何通过索引读取图片及其标签，但要触发MyDataset去读取图片及其标签是在数据加载器DataLoder中 ","date":"2021-10-03","objectID":"/pytorch-note3/:1:2","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（三）数据读取机制与图像预处理模块","uri":"/pytorch-note3/"},{"categories":["Notes"],"content":"1.2DataLoader torch.utils.data.DataLoader(): 构建可迭代的数据装载器, 我们在训练的时候，每一个for循环，每一次迭代，就是从DataLoader中获取一个batch_size大小的数据的。 DataLoader(dataset, batch_size=1, shuffle=False, sampler=None) DataLoader的常用参数： dataset: Dataset类， 决定数据从哪读取以及如何读取 bathsize: 批大小 num_works: 是否多进程读取机制 shuffle: 每个epoch是否乱序 drop_last: 当样本数不能被batch_size整除时， 是否舍弃最后一批数据 2.图像预处理transforms 在实际应用过程中，我们会在数据进入模型之前进行一些预处理，例如数据中心化(仅 减均值)，数据标准化(减均值，再除以标准差)，随机裁剪，旋转一定角度，镜像等一系列 操作。transforms 是常用的图像预处理方法 transforms.Compose方法是将一系列的transforms方法进行有序的组合包装，具体实现的时候，依次的用包装的方法对图像进行操作。 ","date":"2021-10-03","objectID":"/pytorch-note3/:2:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（三）数据读取机制与图像预处理模块","uri":"/pytorch-note3/"},{"categories":["Notes"],"content":"2.1剪裁 transforms.CenterCrop(size)：中心裁剪，size是所需裁剪的尺寸 transforms.RandomCrop：随机裁剪 transforms.RandomResizedCrop ：随机长宽比裁剪 transforms.FiveCrop：上下左右中心裁剪 transforms.TenCrop：上下左右中心裁剪后翻转 ","date":"2021-10-03","objectID":"/pytorch-note3/:3:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（三）数据读取机制与图像预处理模块","uri":"/pytorch-note3/"},{"categories":["Notes"],"content":"2.2翻转和旋转 transforms.RandomHorizontalFlip(p=0.5)：依概率 p 水平翻转 transforms.RandomVerticalFlip(p=0.5)：依概率 p 垂直翻转 transforms.RandomRotation(degrees, resample=False, expand=False, center=None)：随机旋转，degrees表示旋转角度 ， resample表示重采样方法， expand表示是否扩大图片，以保持原图信息。 ","date":"2021-10-03","objectID":"/pytorch-note3/:4:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（三）数据读取机制与图像预处理模块","uri":"/pytorch-note3/"},{"categories":["Notes"],"content":"2.3图片变换 transforms.Resize：重新设定尺寸 transforms.ToTenser：将图像转为 tensor，并归一化至[0-1] transforms.Pad(padding, fill=0, padding_mode=‘constant’): 对图片边缘进行填充 transforms.Grayscale：转灰度图 transforms.Normalize：标准化 transforms.ToPILImage：将图像转换为PILImage ","date":"2021-10-03","objectID":"/pytorch-note3/:5:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（三）数据读取机制与图像预处理模块","uri":"/pytorch-note3/"},{"categories":["Notes"],"content":"2.4选择操作 transforms.RandomChoice(): 从一系列transforms方法中随机选一个 transforms.RandomApply(): 依据概率执行一组transforms操作 transforms.RandomOrder(): 对一组transforms操作打乱顺序 ","date":"2021-10-03","objectID":"/pytorch-note3/:6:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（三）数据读取机制与图像预处理模块","uri":"/pytorch-note3/"},{"categories":["Notes"],"content":"1.计算图与动态图 ","date":"2021-10-01","objectID":"/pytorch-note2/:0:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（二）计算图与自动求梯度","uri":"/pytorch-note2/"},{"categories":["Notes"],"content":"1.1计算图 ","date":"2021-10-01","objectID":"/pytorch-note2/:1:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（二）计算图与自动求梯度","uri":"/pytorch-note2/"},{"categories":["Notes"],"content":"1.1.1定义 随着神经网络层数的加深，各种张量间的计算变得越来越复杂，各个计算之间应该并行执行还是顺序执行？底层设备如何避免重复计算减少冗余？如何提高计算效率？我们使用计算图解决这些问题。 计算图是用于描述计算的有向无环图，我们都知道图包括节点和边，在计算图中节点代表数据，边代表运算 \r","date":"2021-10-01","objectID":"/pytorch-note2/:1:1","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（二）计算图与自动求梯度","uri":"/pytorch-note2/"},{"categories":["Notes"],"content":"1.1.2张量的属性 在计算图中张量有重要的几个属性 （1） is_leaf：叶子节点是直接创建的节点，设置叶子节点的目的是节省内存，在反向传播后，非叶子节点是直接被释放掉的 （2）grad_fn：记录创建张量时所用的具体方法，主要用于梯度求导 ","date":"2021-10-01","objectID":"/pytorch-note2/:1:2","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（二）计算图与自动求梯度","uri":"/pytorch-note2/"},{"categories":["Notes"],"content":"1.2动态图 根据计算图的构建模式，可将计算图分为动态图和静态图 静态图：先搭建图后运算 动态图：运算与搭建同时进行（PyTorch） 2.自动求梯度 参考书籍《动手学深度学习》 在深度学习的任务中，我们经常需要对函数求梯度，PyTorch 提供的 autograd 包能够根据输入的前向传播的过程自动构建计算图，执行反向传播求梯度。 ","date":"2021-10-01","objectID":"/pytorch-note2/:2:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（二）计算图与自动求梯度","uri":"/pytorch-note2/"},{"categories":["Notes"],"content":"2.1autograd包 Tensor 是这个包的核心类，如果将属性 requires_grad 设为 True ，将开始追踪在这个张量上的所有操作，可以利用链式法则传播，最后调用 backward() 方法完全梯度计算，如果不想要被继续追踪，可以调用 detach() 将其从追踪记录中分离出来 注意！在 y.backward() 时，如果 y 是标量，则不需要为 backward() 传入任何参数；否则，需要 传入⼀个与 y 同形的 Tensor ","date":"2021-10-01","objectID":"/pytorch-note2/:3:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（二）计算图与自动求梯度","uri":"/pytorch-note2/"},{"categories":["Notes"],"content":"2.2方法 （1）PyTorch 自动求梯度机制使用的是 torch.autograd.backward 方法，功能是自动求梯度 torch.autograd.backward(tensors, grad_tensors=None, retain_None, create_graph=False) tensors：表示用于求导的张量，如loss retain_graph：表示保存计算图， 由于PyTorch采用了动态图机制，在每一次反向传播结束之后，计算图都会被释放掉。如果我们不想被释放，就要设置这个参数为True create_graph：表示创建导数计算图，用于高阶求导。 grad_tensors：表示多梯度权重。如果有多个loss需要计算梯度的时候，就要设置这些loss的权重比例。 （2）梯度清零：grad在反向传播过程中是累加的，这意味着每一次进行反向传播，梯度都会累加之前的梯度，所以要在反向传播之前需把梯度清零 zero_gard() ","date":"2021-10-01","objectID":"/pytorch-note2/:4:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（二）计算图与自动求梯度","uri":"/pytorch-note2/"},{"categories":["Notes"],"content":"2.3注意 （1）梯度不会自动清零，每一次进行反向传播都会累加之前的梯度，我们在训练神经网络时每进行一次反向传播，都要手动清除梯度 # 清空梯度 optimizer.zero_grad() # 前向传播 output = model(input) # 计算loss batch_loss = loss(output, label) # 反向传播 batch_loss.backward() # 更新参数 optimizer.step() （2）依赖于叶子节点的节点，requires_grad 默认设为 True （3）叶子结点不可执行 in-place（原位操作） in-place操作， 这个操作就是在原始内存当中去改变这个数据，例如列表的 append 操作即原位操作，此过程没有新对象产生 反向传播是根据地址去找 w 的值，如果在反向传播前原位操作将 w 值改变了，反向传播变会出错 ","date":"2021-10-01","objectID":"/pytorch-note2/:5:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（二）计算图与自动求梯度","uri":"/pytorch-note2/"},{"categories":["Notes"],"content":"2.4实例 创建一个 Tensor 并设置 requires_grad=True import torch x = torch.ones(2, 2, requires_grad=True) print(x.grad_fn) # 输出 None 做一次加法运算操作 y = x + 2 print(y.grad\u003cfn) # 输出 \u003cAddBackward0 object at 0x000001AF717C8B0\u003e print(x.is_leaf, y.is_leaf) # True False x 是直接创建的没有 grad_fn ，而 y 是通过加法操作得到的，像 x 这种直接创建的称为叶子节点 z = y * y out = z.mean() 因为 out 是⼀个标量，所以调用 backward() 时不需要指定求导变量 out.backward() print(x.grad) # 输出 tensor([[1.5000, 1.5000], [1.5000, 1.5000]]) 我们分析一下计算过程与求导结果： $$ out=\\frac{1}{4}\\sum_{i=1}^{4}z_{i}=\\frac{1}{4}\\sum_{i=1}^{4}(x_{i}+2)^{2} $$ $$ \\frac{\\partial out}{\\partial x_{i}}=\\frac{3}{2}=1.5 $$ 量都为向量的函数 y = f(x) , 那么 y 关于 x 的梯度就是一个雅可比矩阵，而torch.autograd 这个包就是用来计算雅可比矩阵的乘积 ","date":"2021-10-01","objectID":"/pytorch-note2/:6:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（二）计算图与自动求梯度","uri":"/pytorch-note2/"},{"categories":["Notes"],"content":"1.Tensor的创建 ","date":"2021-09-29","objectID":"/pytorch-note1/:0:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（一）张量的定义与操作","uri":"/pytorch-note1/"},{"categories":["Notes"],"content":"1.1Tensor的简介 （1）张量是一个多维数组，是标量、向量、矩阵的高维扩展，在PyTorch中，torch.Tensor是存储和变换数据的主要工具，它提供了GPU计算的自动求梯度等更多功能 （2）Tensor与Variable：在0.4.0版本后均并入Tensor，Variable有下面五种属性 data: 被包装的Tensor grad: data的梯度 grad_fn: fn表示function的意思，记录我么创建的创建张量时用到的方法 requires_grad: 指示是否需要梯度， 有的不需要梯度 is_leaf: 指示是否是叶子节点（张量） （3）Tensor属性 dtype：张量的数据类型，最常用float32和int64 shape：张量的形状 device：张量所在的设备，CPU或GPU ","date":"2021-09-29","objectID":"/pytorch-note1/:1:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（一）张量的定义与操作","uri":"/pytorch-note1/"},{"categories":["Notes"],"content":"1.2Tensor的创建 （1）直接创建张量torch.Tensor() t = torch.Tensor(data,devicr='cuda') （2）通过numpy数组生成torch.from_numpy() arr = np.array([[1,2,3],[4,5,6]]) t = torch.from_numpy(arr) （3）依据数值创建 torch.zeros() # 创建全零张量 torch.ones() torch.full() # t = torch.full((3,3), 10) torch.arange(start, end, steps) # 创建等差的1维张量,数值区间[start, end)，step表示步长 torch.linspace(start, end, steps) # 创建均分的1维张量,数值区间[start, end]，steps指的是列表的长度 torch.eye() # 创建对角矩阵，默认方阵 （4）依据概率创建张量torch.noraml(mean, std, size, out=None) torch.normal() # mean是均值，std是标准差 torch.randn() # 生成标准正态分布 torch.rand() # 生成均匀分布 torch.randperm(n) # 生成从0 - n-1的随机排列, n是张量的长度 2.Tensor的操作 ","date":"2021-09-29","objectID":"/pytorch-note1/:2:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（一）张量的定义与操作","uri":"/pytorch-note1/"},{"categories":["Notes"],"content":"2.1基本操作 （1）拼接 torch,cat(tensors, dim, out=None) # 将张量按维度dim进行拼接 torch.stack(tensors, dim, out=None) # 在新创建的维度dim上进行拼接 （2）切分 torch.chunk(input, chunks, dim=0) # 将张量按维度dim进行平均切分，返回值是张量列表 torch.split(tensor, split_size_or_sections, dim=0) # 可以指定切分的长度， split_size_or_sections为int时表示每一份的长度 ","date":"2021-09-29","objectID":"/pytorch-note1/:3:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（一）张量的定义与操作","uri":"/pytorch-note1/"},{"categories":["Notes"],"content":"2.2数学操作 TODO ","date":"2021-09-29","objectID":"/pytorch-note1/:4:0","tags":["深度学习","笔记"],"title":"PyTorch学习笔记（一）张量的定义与操作","uri":"/pytorch-note1/"},{"categories":["Web"],"content":"Vue简介 Vue 是一套用于构建用户界面的渐进式框架。Vue对于初学者来说容易上手，官方教程文档也很全面且清楚，是快速建立项目不错的选择。 在学习完Vue的基础后，可以通过使用Vue CLI快速搭建一个属于自己的项目！ CL4配置 Vue 提供了一个CLI，Vue CLI 是一个基于 Vue.js 进行快速开发的完整系统，为单页面应用 (SPA) 快速搭建繁杂的脚手架，CLI4的配置流程大致总结了流程如下： ","date":"2021-04-30","objectID":"/vuecli4/:0:0","tags":["Web"],"title":"Vue CLI4配置","uri":"/vuecli4/"},{"categories":["Web"],"content":"安装node.js 进入node.js官网根据系统安装node.js。安装完node.js之后，npm也会自动安装。查询是否安装成功的命令： node -v npm -v ","date":"2021-04-30","objectID":"/vuecli4/:1:0","tags":["Web"],"title":"Vue CLI4配置","uri":"/vuecli4/"},{"categories":["Web"],"content":"npm换源 使用npm安装一般比较慢，可以安装cnpm淘宝镜像来替换npm cnpm -v ","date":"2021-04-30","objectID":"/vuecli4/:2:0","tags":["Web"],"title":"Vue CLI4配置","uri":"/vuecli4/"},{"categories":["Web"],"content":"安装Vue CLI4 进入终端，全局安装脚手架工具vue-cli，命令如下： npm install --global @vue-cli # OR yarn global add @vue/cli 若你已经安装过久版本，输入如下命令卸载Vue CLI： npm uninstall vue-cli -g 查询是否安装成功及查询版本的命令： Vue -V # OR vue --version ","date":"2021-04-30","objectID":"/vuecli4/:3:0","tags":["Web"],"title":"Vue CLI4配置","uri":"/vuecli4/"},{"categories":["Web"],"content":"开始创建一个项目 在终端输入vue create加项目名称创建一个新的项目： vue create project-name 选择预置，选择Manually select features自定义配置： 至少选择Babel Router Vuex Linter，其他根据需求自主选择： 选择语法检查器，可以默认，也可以选ESlint(语法检查非常严格) 选择In package.json ，使用package.json 项目结构比较简单： 进入项目根目录终端，运行项目：npm run serve，我们发现项目已经运行在本机服务器8080端口 ","date":"2021-04-30","objectID":"/vuecli4/:4:0","tags":["Web"],"title":"Vue CLI4配置","uri":"/vuecli4/"},{"categories":["Web"],"content":"安装axios 在项目中可以选择使用原生ajax发起请求，也可以使用axios，终端进入项目，安装axios： npm --save install axios # OR npm install --save axios vue-axios 将下面代码加入入口main.js文件即可使用： import Vue from 'vue' import axios from 'axios' import VueAxios from 'vue-axios' Vue.use(VueAxios, axios) 项目目总览 ├── src │ ├── App.vue │ ├── assets │ │ └── logo.png │ ├── components # 组件 │ │ └──Helloworld.vue │ ├── views │ │ ├── About.vue │ │ └── Home.vue │ ├── main.js │ ├── router # 路由配置文件 │ │ └── router.js │ └── store │ └── store.js ├── README.md ├── babel.config.js ├── package-lock.json ├── package.json └── babel.config.js 目录简介 ","date":"2021-04-30","objectID":"/vuecli4/:5:0","tags":["Web"],"title":"Vue CLI4配置","uri":"/vuecli4/"},{"categories":["Web"],"content":"public public用于存放静态文件如index.html和favicon.ico public/index.html是项目生成的入口文件，webpack打包的js,css也会自动注入到该页面中。 ","date":"2021-04-30","objectID":"/vuecli4/:6:0","tags":["Web"],"title":"Vue CLI4配置","uri":"/vuecli4/"},{"categories":["Web"],"content":"src src是存放各种vue文件和静态资源的地方 src/assets用于存放各种静态文件，如图片 src/compnents：用于存放公共组件 src/views：用于存放写好的各种页面，如login等 src/App.vue：项目主体vue模块，可以引入其他模块。它是项目的主组件，所有页面都是在app.vue下渲染的 src/main.js：项目的入口文件，主要作用是初始化vue实例，同时可以在此文件中引用某些组件库或者全局挂在一些变量 src/router.js：路由文件，简单来说就是各个页面的地址路径，同时可以直接在里边编写路由守卫 src/store.js：主要用于保存项目的状态 ","date":"2021-04-30","objectID":"/vuecli4/:7:0","tags":["Web"],"title":"Vue CLI4配置","uri":"/vuecli4/"},{"categories":["Web"],"content":"package.json package.json文件包括：项目的基本信息、项目开发所需要模块、版本、项目名称等 ","date":"2021-04-30","objectID":"/vuecli4/:8:0","tags":["Web"],"title":"Vue CLI4配置","uri":"/vuecli4/"},{"categories":null,"content":"About me I am currently a junior majoring in software engineering at University of Electronic Science and Technology of China, member of ZeQi Studio. ","date":"2019-08-02","objectID":"/about/:1:0","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"Education Experience University of Electronic Science and Technology of China Sep. 2019 ~ present B.E. in Software Engineering ","date":"2019-08-02","objectID":"/about/:2:0","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"Awards and Scholarship The First Prize Scholarship 2020 Excellent Student Scholarship 2020,2021 National Second Prize of National College Student Information Security Contest(CISCN) 2021 National Third Prize of The 17th Challenge Cup Special Competition 2021 … ","date":"2019-08-02","objectID":"/about/:3:0","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"Contact Email: yukiyousa@163.com ","date":"2019-08-02","objectID":"/about/:4:0","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"🔗ZeQi Studio: 择栖工作室 🔗Jotang Studio: 焦糖工作室 🔗Vera Xinyue Shen: 数据安全工程师\u0026科幻小说家 优秀学姐 🔗sharifxu: 择栖\u0026FDU 老徐 🔗Klay Sun: 择栖\u0026前端大佬 孙哥哥 🔗yuzhu: 嵌入式\u0026区块链安全 小马哥 🔗su29029: web全栈\u0026安全开发 苏佬 🔗Zhenyu He: THU RA\u0026NLP\u0026DM 何神 🔗rufus: 择栖 六只羊 🔗Yiling He: 信安大赛前辈\u0026ZJU直博 ","date":"2019-08-02","objectID":"/link/:0:0","tags":null,"title":"友链","uri":"/link/"}]